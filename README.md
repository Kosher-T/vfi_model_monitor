# ðŸ›°ï¸ Sentinel: Automated VFI Model Monitoring & Self-Healing Pipeline

## ðŸ—ï¸ The Architecture
*"Models degrade. Good systems heal themselves."*

This project implements an **end-to-end MLOps Drift Detection System** for a Video Frame Interpolation (VFI) model. Instead of manually checking for performance decay (**fondly named "wobbly chairs"**) or data quality issues (**"softwood"**), this system automates the entire **Quality Control loop** using **Containerized Microservices** and **CI/CD Orchestration**.

---

## ðŸš§ Project Status: The Simulation
To test this system properly, I am currently running it in a controlled, **simulated environment**. This allows me to prove that the "safety net" works before trusting it with a live model.

| Component | Status | Notes |
| :--- | :--- | :--- |
| **The Model** | ðŸ”¹ **Simulated** | My actual VFI model is currently still in training. For now, I am using a "mock" (a stand-in) to ensure the monitoring system triggers correctly, regardless of the specific model inside. |
| **Data Source** | ðŸ”¹ **Synthetic** | I created specific test data to "force" the system to react. By feeding it intentionally perfect or intentionally bad data, I can guarantee that the system correctly spots the difference between a **PASS** and a **FAIL**. |
| **Scalability** | ðŸ”¹ **Modular Design** | Currently, this detects drift in **Video** inputs. However, the system is built like building blocks. In the future, I can easily "snap in" new blocks for Audio or Text models without breaking the existing structure. |

---

## ðŸ”„ The Self-Healing Loop (How it Works)

```mermaid
graph TD
    %% --- Nodes ---
    A["â° Daily Schedule"] -->|Trigger| B("GitHub Actions Manager")
    
    %% Manager only has ONE action now: Run the container
    B -->|Run| C
    
    subgraph Phase3 ["Phase 3: The Specialist"]
        direction TB
        C{"ðŸ³ QC Docker Container"}
        
        C -->|Input| D["Incoming Video Data"]
        D -->|"MobileNetV2"| E["Extract Embeddings"]
        E -->|"KS Test"| F["Drift Analysis"]
    end
    
    %% Logic flows out of Phase 3 directly to consequences
    F -->|"Score > 30%"| G["Verdict: FAIL"]
    F -->|"Score < 30%"| H["Verdict: PASS"]
    
    %% Direct sequential actions based on Verdict
    G -->|"Triggers"| J["ðŸš¨ Trigger Retraining Pipeline"]
    H -->|"Action"| K["âœ… Log Success"]
    
    J -->|Simulated| L["Fine-Tune Model on New Data"]
    L --> M["Deploy New Champion"]

    %% --- Styles ---
    classDef base fill:#fff,stroke:#333,stroke-width:1px
    classDef action fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    classDef success fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20
    classDef fail fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#b71c1c
    classDef decision fill:#fff9c4,stroke:#fbc02d,stroke-width:2px

    class A,D,E base
    class B,J,L,M action
    class C,F decision
    class H,K success
    class G fail
```

## ðŸ› ï¸ Tech Stack & Design Patterns

| Component | Tech | Role (The Factory Analogy) |
| :--- | :--- | :--- |
| **Drift Detection** | TensorFlow / **MobileNetV2** | The QC Sensor: Uses Transfer Learning to extract feature **embeddings** from video frames (The "Touch Test"). |
| **Statistical Analysis** | SciPy (**KS Test**) | The Judge: Compares production data against a known "**Golden Baseline**" to quantify distribution shift. |
| **Infrastructure** | **Docker** | The QC Booth: A portable, isolated environment that ensures the monitor runs identically on any machine. |
| **Orchestration** | **GitHub Actions** | The Manager: Automates the schedule, manages the container lifecycle, and handles the "**Red Phone**" logic. |

---

## ðŸš€ The Workflow

### 1. The "Saboteur" (Data Simulation)
To prove the system works, I built a `data_saboteur.py` script that synthetically generates **"drifted" data** (noise, blur, low-light) to simulate real-world camera failures.

### 2. The "Endpoint" (Modular Monitoring)
The monitoring logic is decoupled from the orchestration.

* It runs as a **stateless Docker Container**.
* It accepts a volume of data and a baseline reference.
* It outputs a strictly typed status (`PASS`/`FAIL`) and a **Drift Score**.

> **Why this matters:** This architecture is **model-agnostic**. I can swap the internal logic for an NLP monitor, and the infrastructure remains unchanged.

### 3. The "Red Phone" (Automated Retraining)
When drift is detected (*p-value* < 0.05), the system doesn't just alertâ€”it acts. The primary workflow automatically triggers a secondary **Retraining Pipeline** via the GitHub CLI, simulating a **continuous training (CT) loop**.

---

## ðŸ“¦ How to Run This Project

**Prerequisites:**

* Docker installed
* Python 3.9+

### 1. Build the Monitor:
```bash
docker build -t vfi-monitor .

### 2. Run the Saboteur (Create Bad Data):
```bash
python data_saboteur.py

### 3. Run the QC Check Manually:
docker run \
  -v $(pwd)/data/drifted_frames:/app/incoming_data \
  -v $(pwd)/temp_status:/app/status_output \
  vfi-monitor

## ðŸ”® Future Improvements
* **Dashboarding:** Connect the **Drift Score** output to a **Grafana dashboard** for time-series visualization.
* **A/B Testing:** Implement a **Canary Deployment** strategy for the retrained model.
